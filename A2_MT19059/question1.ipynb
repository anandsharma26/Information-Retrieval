{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "452 452\n",
      "15 15\n",
      "0 0\n",
      "467\n",
      "Enter the query to be searched100 west 53 north\n",
      "Enter the value of no of documents to be retrieved20\n",
      "100 west 53 north\n",
      "['100', 'west', '53', 'north']\n",
      "RETRIEVED DOCUMENTS BASAED ON JACCARD COEFFICINT\n",
      "['100west.txt', 'peace.fun', 'bruce-p.txt', 'arctic.txt', 'dakota.txt', 'poem-2.txt', 'snowmaid.txt', 'weaver.txt', 'candle.hum', 'dicegame.txt', 'charlie.txt', 'campfire.txt', 'bulolli2.txt', 'glimpse1.txt', 'melissa.txt', 'sis', 'lionbird', 'prince.art', 'social.vikings', 'socialvikings.txt']\n",
      "467\n",
      "RETRIEVED DOCUMENTS BASAED ON TF IDF SCORE with TITLE considered same weight\n",
      "['dakota.txt', '100west.txt', 'radar_ra.txt', 'history5.txt', 'vgilante.txt', 'arctic.txt', 'gulliver.txt', 'hound-b.txt', 'quest', 'keepmodu.txt', 'charlie.txt', 'hellmach.txt', 'yukon.txt', 'bruce-p.txt', 'goldbug.poe', 'timem.hac', 'outcast.dos', 'candle.hum', 'long1-3.txt', 'cybersla.txt']\n",
      "RETRIEVED DOCUMENTS BASAED ON TF IDF SCORE WITH TITLE given more weightage\n",
      "['100west.txt', 'dakota.txt', 'spam.key', '13chil.txt', '14.lws', '16.lws', '17.lws', '18.lws', '19.lws', '20.lws', '3gables.txt', '3lpigs.txt', '3sonnets.vrs', '3student.txt', '3wishes.txt', '4moons.txt', '5orange.txt', '6ablemen.txt', '6napolen.txt', '7oldsamr.txt']\n",
      "45098\n",
      "432121\n",
      "RETRIEVED DOCUMENTS BASAED ON COSINE SIMILARITY \n",
      "['100west.txt', 'weaver.txt', 'peace.fun', 'arctic.txt', 'poem-2.txt', 'dicegame.txt', 'charlie.txt', 'sis', 'candle.hum', 'social.vikings', 'socialvikings.txt', 'dakota.txt', 'fic1', 'snowmaid.txt', 'quarter.c3', 'history5.txt', 'assorted.txt', 'goldbug.poe', 'cow.exploder', 'lionbird']\n",
      "45124\n",
      "RETRIEVED DOCUMENTS BASED ON COSINE SIMILARITY WITH TITLE GIVEN MORE WEIGHT\n",
      "['100west.txt', 'arctic.txt', 'dakota.txt', 'charlie.txt', 'spam.key', 'peace.fun', 'candle.hum', 'history5.txt', 'weaver.txt', 'sis', 'dicegame.txt', 'radar_ra.txt', 'socialvikings.txt', 'social.vikings', 'poem-2.txt', 'bruce-p.txt', 'lionbird', 'keepmodu.txt', 'yukon.txt', 'snowmaid.txt']\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "import string\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "\n",
    "def convert_tolowercase(data):\n",
    "    return (data.lower())\n",
    "\n",
    "\n",
    "# In[22]:\n",
    "\n",
    "\n",
    "def regextokenizer_func(data):\n",
    "    #print(type(data))\n",
    "    tokenizer=RegexpTokenizer(r'\\w+')\n",
    "    data=tokenizer.tokenize(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "\n",
    "def remove_stopwords(data):\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    result=[i for i in data if not i in stop_words]\n",
    "    return result\n",
    "\n",
    "\n",
    "# In[24]:\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "def lemmatization_func(data):\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    result=[]\n",
    "    for word in data:\n",
    "        result.append(lemmatizer.lemmatize(word))\n",
    "    return result\n",
    "\n",
    "\n",
    "# In[25]:\n",
    "\n",
    "\n",
    "def stemming_func(data):\n",
    "    stemmer=PorterStemmer()\n",
    "    tokenizer=RegexpTokenizer(r'\\w+')\n",
    "    tokens=tokenizer.tokenize(data)\n",
    "    data_new=\"\"\n",
    "    for i in tokens:\n",
    "        data_new+=\" \"+stemmer.stem(i)\n",
    "    return data_new\n",
    "\n",
    "\n",
    "# In[26]:\n",
    "\n",
    "\n",
    "def jaccard_coefficient(a,b,k,documentname_list):\n",
    "    scores_docid={}\n",
    "    for i in range(len(a)):\n",
    "        count=0\n",
    "        current_doc=a[i]\n",
    "        for j in current_doc:\n",
    "            if j in b:\n",
    "                count+=1\n",
    "        scores_docid[i]=(count/(len(current_doc)+len(b)))\n",
    "    temp=sorted(scores_docid.items(), key=operator.itemgetter(1),reverse=True)\n",
    "    temp1=(temp[:k])\n",
    "    #print(temp1)\n",
    "    retrieved_doc_name=[]\n",
    "    for i in range(k):\n",
    "        docid=temp1[i][0]\n",
    "        retrieved_doc_name.append(documentname_list[docid])\n",
    "    print(\"RETRIEVED DOCUMENTS BASAED ON JACCARD COEFFICINT\")\n",
    "    print(retrieved_doc_name)\n",
    "    \n",
    "\n",
    "\n",
    "# In[27]:\n",
    "\n",
    "\n",
    "def calculate_idf1(temp,N):\n",
    "    return math.log(N/temp)\n",
    "def calculate_idf2(temp,N):\n",
    "    return (math.log(N/(1+temp)))\n",
    "def calculate_idf3(temp,N,maxtemp):\n",
    "    return (math.log(float(maxtemp)/(1+temp)))\n",
    "def calculate_tf1(temp):\n",
    "    return math.log(temp+1)\n",
    "def calculate_tf2(temp):\n",
    "    return (1 if temp>0 else 0)\n",
    "def calculate_tf3(temp,maxtemp):\n",
    "    return (0.5+0.5*(temp/float(maxtemp)))\n",
    "def calculate_tf4(temp,sum_total):\n",
    "    return (temp/sum_total)\n",
    "def calculate_tf5(temp):\n",
    "    return temp\n",
    "\n",
    "\n",
    "# In[28]:\n",
    "\n",
    "\n",
    "title = \"stories\"\n",
    "dataset = []\n",
    "folders = [x[0] for x in os.walk(str(os.getcwd())+'/'+title+'/')]\n",
    "folders[0] = folders[0][:len(folders[0])-1]\n",
    "c = False\n",
    "\n",
    "for i in folders:\n",
    "    file = open(i+\"/index.html\", 'r')\n",
    "    text = file.read().strip()\n",
    "    file.close()\n",
    "\n",
    "    file_name = re.findall('><A HREF=\"(.*)\">', text)\n",
    "    file_title = re.findall('<BR><TD> (.*)\\n', text)\n",
    "\n",
    "    if c == False:\n",
    "        file_name = file_name[2:]\n",
    "        c = True\n",
    "        \n",
    "    print(len(file_name), len(file_title))\n",
    "\n",
    "    for j in range(len(file_name)):\n",
    "        dataset.append((str(i) +\"/\"+ str(file_name[j]), file_title[j]))\n",
    "\n",
    "\n",
    "# In[29]:\n",
    "\n",
    "\n",
    "N=len(dataset)\n",
    "title_list=[]\n",
    "body_list=[]\n",
    "documentname_list=[]\n",
    "for i in ((dataset)):\n",
    "    #print(i)\n",
    "    current_file=open(i[0],'r',encoding='utf-8',errors='ignore')\n",
    "    current_text=current_file.read().strip()\n",
    "    current_title=i[1]\n",
    "    #print(type(current_text))\n",
    "    #print((current_title))\n",
    "    current_text=convert_tolowercase(current_text)\n",
    "    current_title=convert_tolowercase(current_title)\n",
    "    current_text=regextokenizer_func(current_text)\n",
    "    current_title=regextokenizer_func(current_title)\n",
    "    #current_text=remove_stopwords(current_text)\n",
    "    current_text=lemmatization_func(current_text)\n",
    "    #current_text=remove_stopwords(current_text)\n",
    "    current_title=lemmatization_func(current_title)\n",
    "    title_list.append(current_title)\n",
    "    body_list.append(current_text)\n",
    "    loc=i[0]\n",
    "    indexof=loc.rfind('/')\n",
    "    documentname_list.append(i[0][indexof+1:len(loc)])\n",
    "#print(len(title_list))\n",
    "print(len(body_list))\n",
    "#print(documentname_list)\n",
    "\n",
    "\n",
    "# In[30]:\n",
    "\n",
    "\n",
    "def tf_idf_retrieval(title_list,body_list,query,k,documentname_list):\n",
    "    #calculation of DF AND IDF score\n",
    "    df={}\n",
    "    N=len(title_list)\n",
    "    print(N)\n",
    "    #Calculation of DF score\n",
    "    for i in range(N):\n",
    "        tokens1=title_list[i]\n",
    "        tokens2=body_list[i]\n",
    "        #print(tokens1)\n",
    "        #print(tokens2)\n",
    "#         for j in tokens1:\n",
    "#             if j  in df.keys():\n",
    "#                 df[j].add(i)\n",
    "#                 #df[j].append(i)\n",
    "#             else :\n",
    "#                 df[j]={i}\n",
    "        for j in tokens2:\n",
    "            if j in df.keys():\n",
    "                df[j].add(i)\n",
    "            else:\n",
    "                df[j]={i}\n",
    "    #Calculation of idf from df\n",
    "    idf={}\n",
    "    for i in df:\n",
    "        df[i]=len(df[i])\n",
    "    for i in df:\n",
    "        #idf[i]=calculate_idf1(df[i],N)\n",
    "        idf[i]=calculate_idf2(df[i],N)\n",
    "        #idf[i]=calculate_idf3(df[i],N,max(df))\n",
    "    #print(len(idf))\n",
    "    \n",
    "    #calculation of TF SCORE\n",
    "    tf_dict=calculate_tf_score(body_list)\n",
    "    tf_dict1=calculate_tf_score(title_list)\n",
    "    \n",
    "    #print(len(tf_dict))\n",
    "\n",
    "    #calculating tf-idf score\n",
    "    \n",
    "    tf_idf_scores={}\n",
    "    for i in range((N)):\n",
    "        local_score=0\n",
    "        for j in query:\n",
    "            key=(i,j)\n",
    "            if key in tf_dict:\n",
    "                local_score+=(idf.get(j,\"0\")*tf_dict.get(key,\"0\"))\n",
    "        tf_idf_scores[i]=(local_score)\n",
    "        \n",
    "    tf_idf_scores1={}\n",
    "    for i in range((N)):\n",
    "        local_score=0\n",
    "        for j in query:\n",
    "            key=(i,j)\n",
    "            if key in tf_dict1:\n",
    "                temp10=(0.3*float(idf.get(j,\"0\"))*float(tf_dict.get(key,\"0\")))+(0.7*float(idf.get(j,\"0\"))*float(tf_dict1.get(key,\"0\")))\n",
    "                local_score+=temp10\n",
    "        tf_idf_scores1[i]=(local_score)\n",
    "    \n",
    "    print(\"RETRIEVED DOCUMENTS BASAED ON TF IDF SCORE with TITLE considered same weight\")\n",
    "    temp=sorted(tf_idf_scores.items(), key=operator.itemgetter(1),reverse=True)\n",
    "    temp1=(temp[:k])\n",
    "    #print(temp1)\n",
    "    #print((temp1[0]))\n",
    "    retrieved_doc_name=[]\n",
    "    for i in range(k):\n",
    "        docid=temp1[i][0]\n",
    "        retrieved_doc_name.append(documentname_list[docid])\n",
    "    print(retrieved_doc_name)\n",
    "    print(\"RETRIEVED DOCUMENTS BASAED ON TF IDF SCORE WITH TITLE given more weightage\")\n",
    "    temp=sorted(tf_idf_scores1.items(), key=operator.itemgetter(1),reverse=True)\n",
    "    temp1=(temp[:k])\n",
    "    #print(temp1)\n",
    "    #print((temp1[0]))\n",
    "    retrieved_doc_name=[]\n",
    "    for i in range(k):\n",
    "        docid=temp1[i][0]\n",
    "        retrieved_doc_name.append(documentname_list[docid])\n",
    "    print(retrieved_doc_name)\n",
    "\n",
    "\n",
    "# In[31]:\n",
    "\n",
    "\n",
    "def calculate_idf_score(body_list):\n",
    "    N=len(body_list)\n",
    "    df={}\n",
    "    for i in range(N):\n",
    "        tokens2=body_list[i]\n",
    "        for j in tokens2:\n",
    "            if j in df.keys():\n",
    "                df[j].add(i)\n",
    "            else:\n",
    "                df[j]={i}\n",
    "    #Calculation of idf from df\n",
    "    idf={}\n",
    "    for i in df:\n",
    "        df[i]=len(df[i])\n",
    "    for i in df:\n",
    "        idf[i]=calculate_idf1(df[i],N) #inverse document frequency\n",
    "        #idf[i]=calculate_idf2(df[i],N) #inverse document frequency smooth\n",
    "        #idf[i]=calculate_idf3(df[i],N,max(df)) #inverse document frequency max\n",
    "    return idf\n",
    "\n",
    "\n",
    "# In[32]:\n",
    "\n",
    "\n",
    "def calculate_tf_score(body_list):\n",
    "    tf_dict={}\n",
    "    N=len(body_list)\n",
    "    for i in range(N):\n",
    "        tokens2=body_list[i]\n",
    "        terms_counter=Counter(tokens2)\n",
    "        tf_count_list=[]\n",
    "        sum_of_counts=0\n",
    "        for j in tokens2:\n",
    "            sum_of_counts+=terms_counter[j]\n",
    "            tf_count_list.append(terms_counter[j])\n",
    "        for j in tokens2:\n",
    "            count=terms_counter[j]\n",
    "            if j not in tf_dict.keys():\n",
    "                tf_dict[0]=(i)\n",
    "                tf_dict[1]=(j)\n",
    "                tf_dict[i,j]=calculate_tf1(count) #log_normaliztion_TF\n",
    "                #tf_dict[i,j]=calculate_tf2(count) #binary_TF\n",
    "                #tf_dict[i,j]=calculate_tf3(count,max(tf_count_list)) #double normalization TF\n",
    "                #tf_dict[i,j]=calculate_tf4(count,sum_of_counts) #term frequency\n",
    "                #tf_dict[i,j]=calculate_tf5(count) #RAW_TF\n",
    "            else :\n",
    "                tf_dict[i,j].append(calculate_tf1(count))\n",
    "                #tf_dict[i,j].append(calculate_tf2(count))\n",
    "                #tf_dict[i,j].append(calculate_tf3(count,max(tf_count_list)))\n",
    "                #tf_dict[i,j].append(calculate_tf4(count,sum_of_counts))\n",
    "                #tf_dict[i,j].append(calculate_tf5(count))\n",
    "    return tf_dict\n",
    "\n",
    "\n",
    "# In[33]:\n",
    "\n",
    "\n",
    "def calculate_tf_idf_score(tf_dict,idf):\n",
    "    tf_idf_scores={}\n",
    "    for i in range((N)):\n",
    "        for j in idf.keys():\n",
    "            key=(i,j)\n",
    "            if key in tf_dict:\n",
    "                tf_idf_scores[i,j]=(idf.get(j,\"0\")*tf_dict.get(key,\"0\"))\n",
    "    return tf_idf_scores\n",
    "\n",
    "\n",
    "# In[50]:\n",
    "\n",
    "\n",
    "def calculate_tf_idf_score1(tf_dict,tf_dict1,idf):\n",
    "    tf_idf_scores={}\n",
    "    for i in range((N)):\n",
    "        for j in idf.keys():\n",
    "            key=(i,j)\n",
    "            if key in tf_dict:\n",
    "                tf_idf_scores[i,j]=(0.3*float(idf.get(j,\"0\"))*float(tf_dict.get(key,\"0\")))+(0.7*float(idf.get(j,\"0\"))*float(tf_dict1.get(key,\"0\")))\n",
    "    return tf_idf_scores\n",
    "\n",
    "\n",
    "# In[51]:\n",
    "\n",
    "\n",
    "def calculate_tf_idf_query_score(query,idf_document):\n",
    "    idf_document_list=[]\n",
    "    for i in idf_document:\n",
    "        idf_document_list.append(i)\n",
    "    #print(idf_document_list)\n",
    "    term_counter=Counter(query)\n",
    "    query_len=len(query)\n",
    "    query_tf_idf_dict=np.zeros((len(idf_document)))\n",
    "    for i in query:\n",
    "        tf=term_counter[i]/query_len\n",
    "        try:\n",
    "            idf=idf_document[i]\n",
    "        except:\n",
    "            pass\n",
    "        #print(tf*idf)\n",
    "        try:\n",
    "            indexxx=idf_document_list.index(i)\n",
    "            query_tf_idf_dict[indexxx]=tf*idf\n",
    "        except:\n",
    "            pass\n",
    "    #print(query_tf_idf_dict)\n",
    "    return query_tf_idf_dict\n",
    "\n",
    "\n",
    "# In[52]:\n",
    "\n",
    "\n",
    "def cosine_dot(a,b):\n",
    "    if (np.linalg.norm(a)==0 or np.linalg.norm(b)==0):\n",
    "        return 0;\n",
    "    else:\n",
    "        temp=np.dot(a,b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "        return temp\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# In[56]:\n",
    "\n",
    "\n",
    "def cosine_similarity_func(title_list,body_list,query,k,documentname_list):\n",
    "    N=len(body_list)\n",
    "    idf_document=calculate_idf_score(body_list)\n",
    "    idf_document_list=[]\n",
    "    for i in idf_document:\n",
    "        idf_document_list.append(i)\n",
    "    #calculation of TF SCORE document\n",
    "    tf_score_document=calculate_tf_score(body_list)\n",
    "    #tf_score_document1=calculate_tf_score(title_list)\n",
    "    tf_idf_score_document=calculate_tf_idf_score(tf_score_document,idf_document)\n",
    "    #tf_idf_score_document=calculate_tf_idf_score1(tf_score_document,tf_score_document1,idf_document)\n",
    "    print(len(idf_document))\n",
    "    #print(len(tf_score_document))\n",
    "    print(len(tf_idf_score_document))\n",
    "    #TF IDF SCORE FOR QUERY\n",
    "    tf_idf_score_query=calculate_tf_idf_query_score(query,idf_document)\n",
    "    \n",
    "    #print(tf_idf_score_query)\n",
    "    #print(len(tf_idf_score_query))\n",
    "    M=len(idf_document)\n",
    "    matrix=np.zeros((N,M))\n",
    "    for i in tf_idf_score_document:\n",
    "        try:\n",
    "            idx=idf_document_list.index(i[1])\n",
    "            matrix[i[0]][idx]=tf_idf_score_document[i]\n",
    "        except:\n",
    "            pass\n",
    "    #print(len(matrix))\n",
    "    tf_idf_scores={}\n",
    "    for i in range(N):\n",
    "        temp_cos=cosine_dot(tf_idf_score_query,matrix[i])\n",
    "        tf_idf_scores[i]=(temp_cos)\n",
    "    #print(tf_idf_scores)       \n",
    "    temp=sorted(tf_idf_scores.items(), key=operator.itemgetter(1),reverse=True)\n",
    "    temp1=(temp[:k])\n",
    "    #print(temp1)\n",
    "    #print((temp1[0]))\n",
    "    retrieved_doc_name=[]\n",
    "    for i in range(k):\n",
    "        docid=temp1[i][0]\n",
    "        retrieved_doc_name.append(documentname_list[docid])\n",
    "    print(\"RETRIEVED DOCUMENTS BASAED ON COSINE SIMILARITY \")\n",
    "    print(retrieved_doc_name)\n",
    "        \n",
    "\n",
    "\n",
    "# In[57]:\n",
    "\n",
    "\n",
    "def cosine_similarity_func1(title_list,body_list,query,k,documentname_list):\n",
    "    N=len(body_list)\n",
    "    idf_document1=calculate_idf_score(body_list)\n",
    "    idf_document2=calculate_idf_score(title_list)\n",
    "    idf_document={**idf_document1,**idf_document2}\n",
    "    print(len(idf_document))\n",
    "    idf_document_list=[]\n",
    "    for i in idf_document:\n",
    "        idf_document_list.append(i)\n",
    "    #calculation of TF SCORE document\n",
    "    tf_score_document=calculate_tf_score(body_list)\n",
    "    tf_score_document1=calculate_tf_score(title_list)\n",
    "    #tf_idf_score_document=calculate_tf_idf_score(tf_score_document,idf_document)\n",
    "    tf_idf_score_document=calculate_tf_idf_score1(tf_score_document,tf_score_document1,idf_document)\n",
    "    #print(len(idf_document))\n",
    "    #print(len(tf_score_document))\n",
    "    #print(len(tf_idf_score_document))\n",
    "    #TF IDF SCORE FOR QUERY\n",
    "    tf_idf_score_query=calculate_tf_idf_query_score(query,idf_document)\n",
    "    \n",
    "    #print(tf_idf_score_query)\n",
    "    #print(len(tf_idf_score_query))\n",
    "    M=len(idf_document)\n",
    "    matrix=np.zeros((N,M))\n",
    "    for i in tf_idf_score_document:\n",
    "        try:\n",
    "            idx=idf_document_list.index(i[1])\n",
    "            matrix[i[0]][idx]=tf_idf_score_document[i]\n",
    "        except:\n",
    "            pass\n",
    "    #print(len(matrix))\n",
    "    tf_idf_scores={}\n",
    "    for i in range(N):\n",
    "        temp_cos=cosine_dot(tf_idf_score_query,matrix[i])\n",
    "        tf_idf_scores[i]=(temp_cos)\n",
    "    #print(tf_idf_scores)       \n",
    "    temp=sorted(tf_idf_scores.items(), key=operator.itemgetter(1),reverse=True)\n",
    "    temp1=(temp[:k])\n",
    "    #print(temp1)\n",
    "    #print((temp1[0]))\n",
    "    retrieved_doc_name=[]\n",
    "    for i in range(k):\n",
    "        docid=temp1[i][0]\n",
    "        retrieved_doc_name.append(documentname_list[docid])\n",
    "    print(\"RETRIEVED DOCUMENTS BASED ON COSINE SIMILARITY WITH TITLE GIVEN MORE WEIGHT\")\n",
    "    print(retrieved_doc_name)\n",
    "\n",
    "\n",
    "# In[59]:\n",
    "\n",
    "\n",
    "query=input(\"Enter the query to be searched\")\n",
    "k=int(input(\"Enter the value of no of documents to be retrieved\"))\n",
    "#query=\"The Adventure of the adventure\"\n",
    "#query=\"Disco can be fun\"\n",
    "#k=10\n",
    "#query=\"Without the drive of Rebeccah's insistence, Kate lost her momentum. She stood next a slatted oak bench, canisters still clutched, surveying\"\n",
    "#query=\"anand sharma  on the garden\"\n",
    "#query=\"50000 variety of flowers\"\n",
    "if (len(query)==0):\n",
    "    print(\"Empty query so no documents retrieved\")\n",
    "else:\n",
    "    print(query)\n",
    "    query=query.lower()\n",
    "    #print(query)\n",
    "    query=regextokenizer_func(query)\n",
    "    #print(query)\n",
    "    #query=remove_stopwords(query)\n",
    "    query=lemmatization_func(query)\n",
    "    print(query)\n",
    "    jaccard_coefficient(body_list,query,k,documentname_list)\n",
    "    tf_idf_retrieval(title_list,body_list,query,k,documentname_list)\n",
    "    cosine_similarity_func(title_list,body_list,query,k,documentname_list)\n",
    "    cosine_similarity_func1(title_list,body_list,query,k,documentname_list)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
