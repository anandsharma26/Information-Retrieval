{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all libraries \n",
    "import os\n",
    "from os import listdir\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import defaultdict\n",
    "import string\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import statistics\n",
    "import operator\n",
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from num2words import num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "20_newsgroups/sci.med/59199\n",
      "20_newsgroups/comp.graphics/38703\n",
      "20_newsgroups/rec.sport.hockey/53700\n",
      "20_newsgroups/talk.politics.misc/178450\n",
      "20_newsgroups/sci.space/60196\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "# defined a function to retrieve names of file from 20_newsgroup\n",
    "def getListOfFiles(dirName):\n",
    "    listOfFile = os.listdir(dirName)\n",
    "    allFiles = list()\n",
    "    for entry in listOfFile:\n",
    "        fullPath = os.path.join(dirName, entry)\n",
    "        \n",
    "        if os.path.isdir(fullPath):\n",
    "            allFiles = allFiles + getListOfFiles(fullPath)\n",
    "        else:\n",
    "            allFiles.append(fullPath)\n",
    "                \n",
    "    return allFiles\n",
    "\n",
    "\n",
    "path=\"20_newsgroups\"\n",
    "folder_path=getListOfFiles(path)\n",
    "#print((folder_path))\n",
    "#print(\"done\")\n",
    "\n",
    "\n",
    "print(len(folder_path))\n",
    "\n",
    "for i in range (0,5000,1000):\n",
    "    print(folder_path[i])\n",
    "print(len(folder_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defined a function for all the preprocessing\n",
    "def convert_tolowercase(data):\n",
    "    return (data.lower())\n",
    "\n",
    "\n",
    "def regextokenizer_func(data):\n",
    "    #print(type(data))\n",
    "    tokenizer=RegexpTokenizer(r'\\w+')\n",
    "    data=tokenizer.tokenize(data)\n",
    "    return data\n",
    "\n",
    "def remove_stopwords(data):\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    result=[i for i in data if not i in stop_words]\n",
    "    return result\n",
    "\n",
    "\n",
    "def lemmatization_func(data):\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    result=[]\n",
    "    for word in data:\n",
    "        result.append(lemmatizer.lemmatize(word))\n",
    "    return result\n",
    "\n",
    "def stemming_func(data):\n",
    "    stemmer=PorterStemmer()\n",
    "    tokenizer=RegexpTokenizer(r'\\w+')\n",
    "    tokens=tokenizer.tokenize(data)\n",
    "    data_new=\"\"\n",
    "    for i in tokens:\n",
    "        data_new+=\" \"+stemmer.stem(i)\n",
    "    return data_new\n",
    "def convert_numbers(k):\n",
    "    for i in range(len(k)):\n",
    "        try:\n",
    "            k[i] = num2words(int(k[i]))\n",
    "        except:\n",
    "            pass\n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for counting frequency of word in DF dictionary\n",
    "def doc_freq(word):\n",
    "    c = 0\n",
    "    try:\n",
    "        c = DF[word]\n",
    "    except:\n",
    "        pass\n",
    "    return c\n",
    "\n",
    "#DOT product calculation\n",
    "def cosine_dot(a,b):\n",
    "    if (np.linalg.norm(a)==0 or np.linalg.norm(b)==0):\n",
    "        return 0;\n",
    "    else:\n",
    "        temp=np.dot(a,b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "        return temp\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "#opened file and applied all the preprocessing steps and stored in dictionary named body_list\n",
    "total_doc_id=[]\n",
    "body_list=[]\n",
    "doc=0\n",
    "for i in range (0,len(folder_path),1):\n",
    "    text=open(folder_path[i],encoding='utf-8',errors='ignore').read().strip()\n",
    "    text=convert_tolowercase(text)\n",
    "    text=stemming_func(text)\n",
    "    text=regextokenizer_func(text)\n",
    "    text=lemmatization_func(text)\n",
    "    text=remove_stopwords(text)\n",
    "    body_list.append(text)\n",
    "    \n",
    "print(len(body_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56729\n"
     ]
    }
   ],
   "source": [
    "#calculated occurence of words in document DF\n",
    "DF={}\n",
    "cnt=0\n",
    "for tokens in body_list:\n",
    "    for word in (tokens):\n",
    "        try:\n",
    "            DF[word].add(cnt)\n",
    "        except:\n",
    "            DF[word]={cnt}\n",
    "    cnt+=1\n",
    "for i in DF:\n",
    "    DF[i]=len(DF[i])\n",
    "print(len(DF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "836216\n"
     ]
    }
   ],
   "source": [
    "#calculated TF_IDF \n",
    "N=len(body_list)\n",
    "tf_idf={}\n",
    "doc=0\n",
    "for tokens in body_list:\n",
    "    counter=Counter(tokens)\n",
    "    word_len=len(counter)\n",
    "    for word in np.unique(tokens):\n",
    "        tf=counter[word]/word_len\n",
    "        df=doc_freq(word)\n",
    "        idf=np.log(N+1)/df+1\n",
    "        tf_idf[doc,word]=tf*idf\n",
    "    doc+=1\n",
    "print(len(tf_idf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 56729)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Created matrix for calculation of cosine Sim\n",
    "total_vocab_size = len(DF)\n",
    "total_vocab = [x for x in DF]\n",
    "D = np.zeros((N, total_vocab_size),dtype='float16')\n",
    "for i in tf_idf:\n",
    "    try:\n",
    "        ind = total_vocab.index(i[1])\n",
    "        D[i[0]][ind] = tf_idf[i]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defined a query maatrix \n",
    "def query_matrix_func(query):\n",
    "    mat=np.zeros((len(total_vocab)))\n",
    "    counter1=Counter(query)\n",
    "    word_len1=len(counter1)\n",
    "    query_tf_idf={}\n",
    "    for word in np.unique(query):\n",
    "        tf=counter1[word]/word_len1\n",
    "        df=doc_freq(word)\n",
    "        idf=np.log(N+1)/(df+1)\n",
    "        try:\n",
    "            ind=total_vocab.index(word)\n",
    "            mat[ind]=tf*idf\n",
    "        except:\n",
    "            pass\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate cosine scores\n",
    "def cosine_similarity_func(query_mat,k):\n",
    "    cos_sim=[]\n",
    "    for d in D:\n",
    "        cos_sim.append(cosine_dot(query_mat,d))\n",
    "    \n",
    "    outpt=np.array(cos_sim).argsort()[-k:][::-1]\n",
    "    #print(cos_sim)\n",
    "    return list(outpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pretty', 'good', 'opinion', 'biochemistry', 'machine']\n",
      "5\n",
      "Pretty good opinions on biochemistry machines\n",
      "Retrieved Docs are :-  [3305, 973, 2811, 3290, 3981, 2804, 4652, 4968, 2911, 259, 217, 2599, 3426, 978, 75, 3279, 749, 3571, 3352, 2881, 2455, 2871, 3369, 1112, 568, 2817, 1590, 1774, 2367, 2136, 3649, 2874, 2956, 3667, 3791, 2368, 2687, 802, 3360, 3298, 3505, 4848, 611, 3765, 356, 2785, 2373, 3913, 2552, 1271, 852, 101, 2684, 3308, 2052, 1360, 709, 3288, 899, 3728, 12, 2905, 4956, 3376, 944, 4769, 2215, 2835, 2393, 3510, 4053, 4036, 2132, 1642, 59, 2201, 3936, 297, 2851, 3574, 3988, 4377, 2810, 3797, 512, 726, 4193, 13, 4247, 4210, 3962, 912, 3533, 2488, 2679, 1162, 2526, 3655, 1307, 3135]\n"
     ]
    }
   ],
   "source": [
    "#main program\n",
    "#query=\"substantiate my statement\"\n",
    "#query=\"I don't know what kind of machine you want it for, but the program Radiance comes with 'C' source\"\n",
    "#query=\"I claim that I can substantiate my statement that Perot was investigating him.\"\n",
    "query=\"Pretty good opinions on biochemistry machines\"\n",
    "k=100\n",
    "#query=input(\"Enter phrasal query\")\n",
    "#k=input(\"enter k\")\n",
    "orig_query=query\n",
    "query=convert_tolowercase(query)\n",
    "query=regextokenizer_func(query)\n",
    "query=lemmatization_func(query)\n",
    "query=remove_stopwords(query)\n",
    "print((query))\n",
    "query_len=len(query)\n",
    "print(query_len)\n",
    "print(orig_query)\n",
    "query_mat=query_matrix_func(query)\n",
    "retrieved_docs=cosine_similarity_func(query_mat,k)\n",
    "print(\"Retrieved Docs are :- \",retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
